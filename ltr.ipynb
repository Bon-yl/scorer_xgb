{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/adm2/python-scripts/retraining_pipeline/datasets/scorer_dataset_20240829.zip', compression='zip')\n",
    "df['rank'] = df.groupby('you_oid')['rank'].rank(method='first', ascending=True).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttest-ndcg:0.76927\n",
      "[1]\ttest-ndcg:0.77193\n",
      "[2]\ttest-ndcg:0.77193\n",
      "[3]\ttest-ndcg:0.77758\n",
      "[4]\ttest-ndcg:0.77730\n",
      "[5]\ttest-ndcg:0.77819\n",
      "[6]\ttest-ndcg:0.77861\n",
      "[7]\ttest-ndcg:0.78042\n",
      "[8]\ttest-ndcg:0.78032\n",
      "[9]\ttest-ndcg:0.78031\n",
      "[10]\ttest-ndcg:0.78052\n",
      "[11]\ttest-ndcg:0.78077\n",
      "[12]\ttest-ndcg:0.78077\n",
      "[13]\ttest-ndcg:0.78126\n",
      "[14]\ttest-ndcg:0.78122\n",
      "[15]\ttest-ndcg:0.78124\n",
      "[16]\ttest-ndcg:0.78127\n",
      "[17]\ttest-ndcg:0.78125\n",
      "[18]\ttest-ndcg:0.78161\n",
      "[19]\ttest-ndcg:0.78204\n",
      "[20]\ttest-ndcg:0.78274\n",
      "[21]\ttest-ndcg:0.78275\n",
      "[22]\ttest-ndcg:0.78356\n",
      "[23]\ttest-ndcg:0.78392\n",
      "[24]\ttest-ndcg:0.78394\n",
      "[25]\ttest-ndcg:0.78414\n",
      "[26]\ttest-ndcg:0.78419\n",
      "[27]\ttest-ndcg:0.78443\n",
      "[28]\ttest-ndcg:0.78388\n",
      "[29]\ttest-ndcg:0.78397\n",
      "[30]\ttest-ndcg:0.78379\n",
      "[31]\ttest-ndcg:0.78376\n",
      "[32]\ttest-ndcg:0.78389\n",
      "[33]\ttest-ndcg:0.78389\n",
      "[34]\ttest-ndcg:0.78404\n",
      "[35]\ttest-ndcg:0.78453\n",
      "[36]\ttest-ndcg:0.78425\n",
      "[37]\ttest-ndcg:0.78412\n",
      "[38]\ttest-ndcg:0.78412\n",
      "[39]\ttest-ndcg:0.78404\n",
      "[40]\ttest-ndcg:0.78417\n",
      "[41]\ttest-ndcg:0.78411\n",
      "[42]\ttest-ndcg:0.78377\n",
      "[43]\ttest-ndcg:0.78380\n",
      "[44]\ttest-ndcg:0.78415\n",
      "[45]\ttest-ndcg:0.78474\n",
      "[46]\ttest-ndcg:0.78461\n",
      "[47]\ttest-ndcg:0.78463\n",
      "[48]\ttest-ndcg:0.78480\n",
      "[49]\ttest-ndcg:0.78483\n",
      "[50]\ttest-ndcg:0.78490\n",
      "[51]\ttest-ndcg:0.78539\n",
      "[52]\ttest-ndcg:0.78515\n",
      "[53]\ttest-ndcg:0.78526\n",
      "[54]\ttest-ndcg:0.78515\n",
      "[55]\ttest-ndcg:0.78560\n",
      "[56]\ttest-ndcg:0.78548\n",
      "[57]\ttest-ndcg:0.78561\n",
      "[58]\ttest-ndcg:0.78556\n",
      "[59]\ttest-ndcg:0.78539\n",
      "[60]\ttest-ndcg:0.78543\n",
      "[61]\ttest-ndcg:0.78582\n",
      "[62]\ttest-ndcg:0.78583\n",
      "[63]\ttest-ndcg:0.78597\n",
      "[64]\ttest-ndcg:0.78590\n",
      "[65]\ttest-ndcg:0.78590\n",
      "[66]\ttest-ndcg:0.78592\n",
      "[67]\ttest-ndcg:0.78590\n",
      "[68]\ttest-ndcg:0.78597\n",
      "[69]\ttest-ndcg:0.78601\n",
      "[70]\ttest-ndcg:0.78653\n",
      "[71]\ttest-ndcg:0.78630\n",
      "[72]\ttest-ndcg:0.78638\n",
      "[73]\ttest-ndcg:0.78629\n",
      "[74]\ttest-ndcg:0.78633\n",
      "[75]\ttest-ndcg:0.78624\n",
      "[76]\ttest-ndcg:0.78628\n",
      "[77]\ttest-ndcg:0.78636\n",
      "[78]\ttest-ndcg:0.78644\n",
      "[79]\ttest-ndcg:0.78657\n",
      "[80]\ttest-ndcg:0.78660\n",
      "[81]\ttest-ndcg:0.78645\n",
      "[82]\ttest-ndcg:0.78620\n",
      "[83]\ttest-ndcg:0.78629\n",
      "[84]\ttest-ndcg:0.78643\n",
      "[85]\ttest-ndcg:0.78635\n",
      "[86]\ttest-ndcg:0.78656\n",
      "[87]\ttest-ndcg:0.78651\n",
      "[88]\ttest-ndcg:0.78647\n",
      "[89]\ttest-ndcg:0.78649\n",
      "[90]\ttest-ndcg:0.78651\n",
      "Predicted rankings: [0.46892098 0.3853441  0.3158477  ... 0.27451342 0.51563734 0.10982303]\n",
      "NDCG score: 0.2902952056941294\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# Features and labels\n",
    "X = df[['color_score', 'openai_score', 'image_score', 'mobilenet_score']]  # Feature columns\n",
    "y = df['rank']  # Ranking labels\n",
    "\n",
    "# Group sizes (5 per group since you have top 5 ranks for each 'you_oid')\n",
    "groups = df.groupby('you_oid').size().values\n",
    "\n",
    "# Group-based train-test split\n",
    "gss = GroupShuffleSplit(test_size=0.1, n_splits=1, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(X, y, groups=df['you_oid']))\n",
    "\n",
    "X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "groups_train = df.iloc[train_idx].groupby('you_oid').size().values\n",
    "groups_test = df.iloc[test_idx].groupby('you_oid').size().values\n",
    "\n",
    "# Create DMatrix for XGBoost\n",
    "train_dmatrix = xgb.DMatrix(X_train, label=y_train)\n",
    "test_dmatrix = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Set group sizes for the training and test sets\n",
    "train_dmatrix.set_group(groups_train)\n",
    "test_dmatrix.set_group(groups_test)\n",
    "\n",
    "# Train the XGBoost model\n",
    "params = {\n",
    "    'objective': 'rank:ndcg',\n",
    "    'eval_metric': 'ndcg',\n",
    "    'eta': 0.1,\n",
    "    'max_depth': 6,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "bst = xgb.train(params, train_dmatrix, num_boost_round=500, evals=[(test_dmatrix, 'test')], early_stopping_rounds=10)\n",
    "\n",
    "# Predict ranking for the test set\n",
    "pred = bst.predict(test_dmatrix)\n",
    "\n",
    "# Display predictions\n",
    "print(\"Predicted rankings:\", pred)\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "# Calculate NDCG score\n",
    "ndcg = ndcg_score([y_test], [pred], k=5)\n",
    "print(\"NDCG score:\", ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         you_oid  pred_score  pred_rank\n",
      "126     5f640b3b051a932d5762b433    0.468921          2\n",
      "127     5f640b3b051a932d5762b433    0.385344          4\n",
      "128     5f640b3b051a932d5762b433    0.315848          5\n",
      "129     5f640b3b051a932d5762b433    0.177788          6\n",
      "130     5f640b3b051a932d5762b433    0.108562          7\n",
      "...                          ...         ...        ...\n",
      "521316  66ce56b2b94a5511fe0be08f    0.810955          1\n",
      "521317  66ce56b2b94a5511fe0be08f    0.647703          2\n",
      "521318  66ce56b2b94a5511fe0be08f    0.274513          4\n",
      "521319  66ce56b2b94a5511fe0be08f    0.515637          3\n",
      "521320  66ce56b2b94a5511fe0be08f    0.109823          5\n",
      "\n",
      "[52077 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you already have your X_test and y_pred as in previous steps\n",
    "\n",
    "# Convert predictions to a DataFrame for easier manipulation\n",
    "pred_df = pd.DataFrame({\n",
    "    'you_oid': df['you_oid'].iloc[test_idx],  # Grouping key\n",
    "    'pred_score': pred                      # Predicted scores from the model\n",
    "})\n",
    "\n",
    "# Group by `you_oid` and rank the `pred_score` within each group\n",
    "pred_df['pred_rank'] = pred_df.groupby('you_oid')['pred_score'].rank(method='first', ascending=False).astype(int)\n",
    "\n",
    "# Display the predictions with ranks\n",
    "print(pred_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126       5\n",
       "127       6\n",
       "128       1\n",
       "129       2\n",
       "130       3\n",
       "         ..\n",
       "521316    4\n",
       "521317    3\n",
       "521318    1\n",
       "521319    5\n",
       "521320    2\n",
       "Name: rank, Length: 52077, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.16      0.16      0.16     10006\n",
      "           2       0.17      0.17      0.17      9541\n",
      "           3       0.21      0.21      0.21      9351\n",
      "           4       0.17      0.17      0.17      9183\n",
      "           5       0.14      0.14      0.14      9034\n",
      "           6       0.09      0.09      0.09      2626\n",
      "           7       0.08      0.08      0.08       989\n",
      "           8       0.09      0.09      0.09       524\n",
      "           9       0.04      0.04      0.04       332\n",
      "          10       0.06      0.06      0.06       258\n",
      "          11       0.02      0.02      0.02       128\n",
      "          12       0.00      0.00      0.00        54\n",
      "          13       0.06      0.06      0.06        32\n",
      "          14       0.00      0.00      0.00        13\n",
      "          15       0.00      0.00      0.00         5\n",
      "          16       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.16     52077\n",
      "   macro avg       0.08      0.08      0.08     52077\n",
      "weighted avg       0.16      0.16      0.16     52077\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, pred_df['pred_rank']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged DataFrame with True Ranks and Predicted Ranks:\n",
      "                         you_oid  rank  pred_rank\n",
      "0       5f640b3b051a932d5762b433     5          2\n",
      "1       5f640b3b051a932d5762b433     5          4\n",
      "2       5f640b3b051a932d5762b433     5          5\n",
      "3       5f640b3b051a932d5762b433     5          6\n",
      "4       5f640b3b051a932d5762b433     5          7\n",
      "...                          ...   ...        ...\n",
      "296372  66ce56b2b94a5511fe0be08f     2          1\n",
      "296373  66ce56b2b94a5511fe0be08f     2          2\n",
      "296374  66ce56b2b94a5511fe0be08f     2          4\n",
      "296375  66ce56b2b94a5511fe0be08f     2          3\n",
      "296376  66ce56b2b94a5511fe0be08f     2          5\n",
      "\n",
      "[296377 rows x 3 columns]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Only ('multilabel-indicator', 'continuous-multioutput', 'multiclass-multioutput') formats are supported. Got binary instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m     true_ranks \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     22\u001b[0m     pred_ranks \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_rank\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m---> 23\u001b[0m     ndcg \u001b[38;5;241m=\u001b[39m \u001b[43mndcg_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrue_ranks\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mpred_ranks\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     ndcg_scores\u001b[38;5;241m.\u001b[39mappend(ndcg)\n\u001b[1;32m     26\u001b[0m mean_ndcg \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(ndcg_scores)\n",
      "File \u001b[0;32m~/.virtualenvs/scorer/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:1618\u001b[0m, in \u001b[0;36mndcg_score\u001b[0;34m(y_true, y_score, k, sample_weight, ignore_ties)\u001b[0m\n\u001b[1;32m   1616\u001b[0m y_score \u001b[38;5;241m=\u001b[39m check_array(y_score, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1617\u001b[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[0;32m-> 1618\u001b[0m \u001b[43m_check_dcg_target_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1619\u001b[0m gain \u001b[38;5;241m=\u001b[39m _ndcg_sample_scores(y_true, y_score, k\u001b[38;5;241m=\u001b[39mk, ignore_ties\u001b[38;5;241m=\u001b[39mignore_ties)\n\u001b[1;32m   1620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39maverage(gain, weights\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m~/.virtualenvs/scorer/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:1355\u001b[0m, in \u001b[0;36m_check_dcg_target_type\u001b[0;34m(y_true)\u001b[0m\n\u001b[1;32m   1349\u001b[0m supported_fmt \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-indicator\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1351\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontinuous-multioutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1352\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass-multioutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1353\u001b[0m )\n\u001b[1;32m   1354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m supported_fmt:\n\u001b[0;32m-> 1355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1356\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m formats are supported. Got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m instead\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1357\u001b[0m             supported_fmt, y_type\n\u001b[1;32m   1358\u001b[0m         )\n\u001b[1;32m   1359\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Only ('multilabel-indicator', 'continuous-multioutput', 'multiclass-multioutput') formats are supported. Got binary instead"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "# Assuming `df` is your original DataFrame with the true ranks\n",
    "# and `pred_df` contains the new predicted ranks.\n",
    "\n",
    "# Merge the true ranks with the predicted ranks\n",
    "evaluation_df = pd.merge(df[['you_oid', 'rank']], pred_df[['you_oid', 'pred_rank']], on='you_oid')\n",
    "\n",
    "# Display the merged DataFrame\n",
    "print(\"Merged DataFrame with True Ranks and Predicted Ranks:\")\n",
    "print(evaluation_df)\n",
    "\n",
    "# Evaluate Mean NDCG\n",
    "# Group by `you_oid` to compute NDCG for each group\n",
    "groups = evaluation_df.groupby('you_oid')\n",
    "ndcg_scores = []\n",
    "\n",
    "for _, group in groups:\n",
    "    true_ranks = group['rank'].values\n",
    "    pred_ranks = group['pred_rank'].values\n",
    "    ndcg = ndcg_score([true_ranks], [pred_ranks])\n",
    "    ndcg_scores.append(ndcg)\n",
    "\n",
    "mean_ndcg = np.mean(ndcg_scores)\n",
    "print(f'Mean NDCG Score: {mean_ndcg}')\n",
    "\n",
    "# Evaluate Mean Reciprocal Rank (MRR)\n",
    "reciprocal_ranks = []\n",
    "\n",
    "for _, group in groups:\n",
    "    # Sort by predicted rank to find the true rank of the top prediction\n",
    "    sorted_group = group.sort_values(by='pred_rank')\n",
    "    # Get the rank of the first item in sorted predictions\n",
    "    true_rank_of_top_pred = sorted_group['rank'].iloc[0]\n",
    "    reciprocal_rank = 1 / true_rank_of_top_pred\n",
    "    reciprocal_ranks.append(reciprocal_rank)\n",
    "\n",
    "mean_mrr = np.mean(reciprocal_ranks)\n",
    "print(f'Mean Reciprocal Rank (MRR): {mean_mrr}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scorer",
   "language": "python",
   "name": "scorer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
